<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Final Report</title>
    <link rel="stylesheet" href="../css/proposal.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <div class="overlay"></div>
    <nav class="side-nav">
        <ul>
            <li><a href="#title">Top</a></li>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#approach">Approach</a></li>
            <li><a href="#experiments">Experiments and Results</a></li>
            <li><a href="#qual-results">Qualitative Results</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </nav>

    <section class="title" id="title">
        <h1>Final Report: Detecting Distracted Driving Using Computer Vision Techniques</h1>
        <div class="subtitles">
            <p>Aziz Shaik and Rayden Dodd</p>
            <p>ECE 5554: Computer Vision (Fall 2025)</p>
            <p>Virginia Tech</p>
        </div>
    </section>

    <section>
        <!-- Replace this image with something visual from this project -->
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/final2221.png" alt="Distracted Driving" style="width:300px; height:auto;">
                <figcaption>A Distracted Driver on the Phone</figcaption>
            </figure>
            <figure>
                <img src="./static/deep_learning/final2221.png" alt="Safe Driving" style="width:300px; height:auto;">
                <figcaption>A Safe Driver</figcaption>
            </figure>
        </div>
    </section>

    <section class="section" id="abstract">
        <h2>Abstract</h2>
        <p>
            Modern vehicles come equipped with a number of sensors for safe automotive operation, but these measures tend to be reactive
            to driver behavior rather than proactive. Using computer vision techniques, such as CNNs and optical flow methods with landmarks, we have
            developed two different approaches to capturing and identifying unsafe driver behavior for proactive in-vehicle safety
            interventions. Using deep learning methods, we obtained a maximum of 96% test accuracy with a RESNET based classifier and
            92.7% with a custom CNN. While specific accuracy statistics were not feasible due to the qualitative method of the landmark and optical
            flow method, it performed reasonably well in a live environment, but did require a lot of threshold tuning which was sometimes inconsistent.
        </p>
    </section>

    <section class="section" id="Introduction">
        <h2>Introduction</h2>
        <p>
            Distracted driving is a leading cause of accidents around the world, posing a serious threat to public safety.
            According to the Centers for Disease Control and Prevention (CDC), over 3,100 individuals were killed and 
            approximately 424,000 were injured in motor vehicle crashes involving a distracted driver in the United States in 2019 [1]. 
            Equating to almost nine deaths per day, there is a very present need to solve this problem.
            Despite this, studies indicate that 68% of Gen Z drivers admit to using mobile devices while operating a vehicle [2], 
            highlighting a disconnect between the awareness of the risk and the actual driving behavior.
        </p>
        <p>
            Many modern vehicles incorporate systems to ensure drivers are focused by limiting interface access or flashing a warning when
            it detects irregular driving. However, these systems primarily respond to interactions with the vehicle itself and often 
            fail to capture the broader spectrum of risky driving behaviors.
        </p>
        <p>
            The core challenge presented by this problem lies in accurately identifying specific driver behaviors in real time using visual 
            data, with operator actions such as phone usage, eating, or signs of tiredness, and making intelligent decisions based on the 
            severity of the behavior. A system capable of detecting these patterns could help deter dangerous conduct and contribute to a 
            safer travel experience for passengers and nearby drivers.
        </p>
    </section>

    <section class="section" id="approach">
        <h2>Approach</h2>
        <p>
            For this problem, we attempt two different technical approaches: one method using deep learning model to classify behavior and another method making use
            of landmarks and optical flow analysis to characterize behavior. The approach differs for each with different success criteria for each method.
        </p>
        <h3>Deep Learning</h3>
        <p>
            The deep learning approach involved training two different convolutional neural networks (CNNs)
            to classify images of drivers into 6 different categories based on their behavior.
            This method leverages the power of deep learning to automatically learn features from the images
            that are indicative of distracted driving.
        </p>

        <h4>Preprocessing</h4>
        <p>
            All images are first converted from raw pixel data into PyTorch tensors and resized to a consistent
            resolution of <code>224 × 224</code> pixels. This unified resolution makes the data compatible with
            both the custom CNN and the ResNet-based model.
        </p>
        <p>
            To improve generalization, the training set is augmented with random transformations such as:
        </p>
        <ul>
            <li>Random horizontal flips</li>
            <li>Random rotations</li>
            <li>Color jitter (brightness and contrast)</li>
            <li>Gaussian blur</li>
        </ul>

        <h5>Custom CNN Preprocessing</h5>
        <p>
            For the custom CNN, the dataset statistics (mean and standard deviation) are computed directly from
            the training images using <code>get_image_means_stds</code>. These values are then used to normalize
            each input image, which helps stabilize training by putting all channels on a similar scale.
        </p>

        <h5>ResNet-50 Preprocessing</h5>
        <p>
            For the transfer learning setup with ResNet-50, the images are normalized using the standard
            ImageNet statistics (<code>mean = [0.485, 0.456, 0.406]</code>,
            <code>std = [0.229, 0.224, 0.225]</code>). This matches the preprocessing used when ResNet-50 was
            originally trained on ImageNet, allowing the pretrained weights to transfer effectively to the
            distracted driving task.
        </p>

        <h4>Model Architectures</h4>

        <h5>Custom CNN</h5>
        <p>
            The custom CNN is a relatively lightweight architecture built from four convolutional layers with
            batch normalization and max pooling, followed by an adaptive average pooling layer and three fully
            connected layers. The convolutional layers extract increasingly abstract spatial features from the
            input images, and the fully connected layers map these features to the 6 driver behavior classes.
        </p>
        <div class="cnn-architecture">
            <table>
                <thead>
                    <tr>
                        <th>Layer (type)</th>
                        <th>Output Shape</th>
                        <th>Param #</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Conv2d-1</td>
                        <td>[-1, 32, 224, 224]</td>
                        <td>896</td>
                    </tr>
                    <tr>
                        <td>BatchNorm2d-2</td>
                        <td>[-1, 32, 224, 224]</td>
                        <td>64</td>
                    </tr>
                    <tr>
                        <td>Conv2d-3</td>
                        <td>[-1, 64, 224, 224]</td>
                        <td>18,496</td>
                    </tr>
                    <tr>
                        <td>BatchNorm2d-4</td>
                        <td>[-1, 64, 224, 224]</td>
                        <td>128</td>
                    </tr>
                    <tr>
                        <td>MaxPool2d-5</td>
                        <td>[-1, 64, 112, 112]</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>Conv2d-6</td>
                        <td>[-1, 128, 112, 112]</td>
                        <td>73,856</td>
                    </tr>
                    <tr>
                        <td>BatchNorm2d-7</td>
                        <td>[-1, 128, 112, 112]</td>
                        <td>256</td>
                    </tr>
                    <tr>
                        <td>Conv2d-8</td>
                        <td>[-1, 128, 112, 112]</td>
                        <td>147,584</td>
                    </tr>
                    <tr>
                        <td>BatchNorm2d-9</td>
                        <td>[-1, 128, 112, 112]</td>
                        <td>256</td>
                    </tr>
                    <tr>
                        <td>MaxPool2d-10</td>
                        <td>[-1, 128, 56, 56]</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>AdaptiveAvgPool2d-11</td>
                        <td>[-1, 128, 14, 14]</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>Linear-12</td>
                        <td>[-1, 512]</td>
                        <td>12,845,568</td>
                    </tr>
                    <tr>
                        <td>Linear-13</td>
                        <td>[-1, 128]</td>
                        <td>65,664</td>
                    </tr>
                    <tr>
                        <td>Linear-14</td>
                        <td>[-1, 6]</td>
                        <td>774</td>
                    </tr>
                </tbody>
            </table>
            <p><strong>Total params:</strong> 13,153,542</p>
            <p><strong>Trainable params:</strong> 13,153,542</p>
            <p><strong>Non-trainable params:</strong> 0</p>
            <p><strong>Input size (MB):</strong> 0.57</p>
            <p><strong>Forward/backward pass size (MB):</strong> 131.88</p>
            <p><strong>Params size (MB):</strong> 50.18</p>
            <p><strong>Estimated total size (MB):</strong> 182.63</p>
        </div>

        <h5>ResNet-50 (Transfer Learning)</h5>
        <p>
            The second model is based on a ResNet-50 architecture pretrained on ImageNet. The original
            final fully connected (FC) layer is replaced with a new <code>nn.Linear</code> layer that
            outputs 6 logits, one for each driver behavior class. To adapt the model to this task while
            preserving most of the pretrained features, only the deepest residual block (<code>layer4</code>)
            and the final FC layer are allowed to update during training; all earlier layers are frozen.
        </p>
        <p>
            This fine-tuning strategy allows ResNet-50 to reuse its low-level and mid-level features
            (edges, textures, shapes) from ImageNet, while the high-level features in <code>layer4</code>
            and the classifier are updated to specialize in distracted driving detection.
        </p>

        <div class="cnn-architecture">
            <table>
                <thead>
                    <tr>
                        <th>Layer (type)</th>
                        <th>Output Shape</th>
                        <th>Param #</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Conv2d-1</td><td>[-1, 64, 112, 112]</td><td>9,408</td></tr>
                    <tr><td>BatchNorm2d-2</td><td>[-1, 64, 112, 112]</td><td>128</td></tr>
                    <tr><td>ReLU-3</td><td>[-1, 64, 112, 112]</td><td>0</td></tr>
                    <tr><td>MaxPool2d-4</td><td>[-1, 64, 56, 56]</td><td>0</td></tr>
                    <tr><td>...</td><td>...</td><td>...</td></tr>
                    <tr><td>AdaptiveAvgPool2d-173</td><td>[-1, 2048, 1, 1]</td><td>0</td></tr>
                    <tr><td>Linear-174</td><td>[-1, 6]</td><td>12,294</td></tr>
                    <tr><td>ResNet-175</td><td>[-1, 6]</td><td>0</td></tr>
                </tbody>
            </table>
            <p><strong>Total params:</strong> 23,520,326</p>
            <p><strong>Trainable params:</strong> 14,977,030</p>
            <p><strong>Non-trainable params:</strong> 8,543,296</p>
            <p><strong>Input size (MB):</strong> 0.57</p>
            <p><strong>Forward/backward pass size (MB):</strong> 286.55</p>
            <p><strong>Params size (MB):</strong> 89.72</p>
            <p><strong>Estimated total size (MB):</strong> 376.85</p>
        </div>

        <h4>Training</h4>

        <h5>Custom CNN Training</h5>
        <p>
            The custom CNN is trained from scratch using mini-batches of size <code>16</code> for
            <code>20</code> epochs. At each iteration, a batch of normalized images is passed through
            the network, the cross-entropy loss (with <code>label_smoothing = 0.05</code>) is computed,
            and the gradients are backpropagated. The optimizer is stochastic gradient descent (SGD)
            with a learning rate of <code>0.001</code>, momentum <code>0.9</code>, and weight decay
            <code>1e-4</code>, which helps the model converge while reducing overfitting.
        </p>
        <p>
            Data loading is parallelized with <code>num_workers = 4</code>, and each epoch corresponds
            to a full pass through the training set. Training and validation losses and accuracies are
            recorded so that the learning curves can be visualized after training.
        </p>

        <h5>ResNet-50 Training</h5>
        <p>
            The modified ResNet-50 model is trained using transfer learning. The network is initialized
            with ImageNet-pretrained weights, and only the parameters in <code>layer4</code> and the
            final <code>fc</code> layer are updated. Training also runs for <code>20</code> epochs with
            mini-batches of size <code>16</code>, but the optimization setup differs from the custom CNN.
        </p>
        <p>
            An Adam optimizer is used with separate learning rates: a slightly higher rate for the new
            fully connected layer and a smaller rate for <code>layer4</code>. This allows the new
            classifier to adapt quickly while making more conservative updates to the pretrained
            convolutional features. A smaller amount of label smoothing is applied than in the custom CNN,
            reflecting the fact that ResNet-50 starts from a strong pretrained representation.
        </p>

        <h4>Evaluation</h4>
        <p>
            After each training epoch, both models are evaluated on a separate validation set without
            updating any weights. The models are switched to evaluation mode, and metrics such as
            accuracy, precision, recall, and F1-score (macro-averaged across the 6 classes) are computed,
            along with the validation loss.
        </p>
        <p>
            These validation metrics are used to monitor for overfitting: if training accuracy continues
            to improve while validation accuracy or F1-score stagnates or decreases, the model is likely
            memorizing the training data. The model checkpoint that achieves the best validation
            accuracy is saved to disk and treated as the “best” version for later testing.
        </p>

        <h4>Testing</h4>
        <p>
            Once training is complete, the best-performing checkpoint for each model (based on validation
            accuracy) is evaluated on a held-out test set that was never used during training or validation.
            This final evaluation provides an unbiased estimate of how well each model can classify driver
            behavior in real-world scenarios.
        </p>
        <p>
            The same metrics accuracy, precision, recall, and F1-score are reported on the test set,
            allowing a direct comparison between the performance of the custom CNN and the transfer-learning
            ResNet-50 model.
        </p>

        <h3>Landmarks and Optical Flow Analysis</h3>
        <p>
            As an alternative to a pure deep learning procedure, we implement a motion-based approach using optical flow to detect movements using
            facial landmarks. This method analyzes frame-to-frame changes in video captured from a live camera feed or video, 
            allowing us to track  head movement, eye movement, and other facial features without relying on labeled image classification.
        </p>
        <h4>Face Detection</h4>
        <ul>
            <li>Use MediaPipe's pre-trained facial detection model to efficiently locate bounding boxes around the driver's face in each frame</li>
            <li>If there are multiple faces found, select the largest one to filter out other faces and possible false detections.</li>
            <li>Capture the dimensions of this bounding box as the region of interest to work with.</li>
        </ul>
        <br>
        <p>
            Initially, the Haar cascade classifier was used for facial detection, and while it was very fast, it tended to be inaccurate and the bounding box
            that was returned was often to small and did not capture enough of the face. Additionally, there would be many frames where it simply did not capture
            any faces. From a sample of 4089 frames, the Haar cascade classifier could not locate a face in 1315 of these frames, while MediaPipe only missed 72 frames.
        </p>
            
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/facebox_frame_2911_haar.png" alt="Haar" style="width:400px; height:auto;">
                <figcaption>Bounding Box with Haar Cascades</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/facebox_frame_2911_mp.png" alt="MediaPipe" style="width:400px; height:auto;">
                <figcaption>Bounding Box with MediaPipe</figcaption>
            </figure>
        </div>

        <p>
            With the bounding box, we could ignore anything outside of it and use this to reduce computation and possible errors. If there are multiple people in the car,
            the main face in front of the camera, the driver, would be the only one with a facial mesh formed for it, further simplifying computation.
        </p>

        <h4>Optical Flow Analysis</h4>
        <ul>
            <li>Extract facial landmarks from consecutive frames using MediaPipe's face mesh.</li>
            <li>Apply Lucas-Kanade optical flow (cv2.calcOpticalFlowPyrLK) to track landmark displacement between frames.</li>
            <li>Compute average horizontal (dx) and vertical (dy) shifts to infer head movement.</li>
            <li>Threshold displacements to classify transitions in one of the following states:
                <ul>
                    <li>turning left</li>
                    <li>turning right</li>
                    <li>looking up</li>
                    <li>looking down</li>
                </ul>
            </li>
        </ul>
        <br>
        <p>
            The initial approach involved making use of Farneback optical flow, but even with the cropped ROI, this was far too computationally expensive. Running live
            was very slow and eventually crashed, and running with an MP4 file caused crashes as well, doing about 2000 frames in 20 minutes. For this reason, we switched
            to using sparse optical flow, the Lucas-Kanade algorithm, which ran much faster and was usable for live detection. The initial implementation made use of
            cv2.goodFeaturesToTrack, which uses the Shi-Tomasi corner detection algorithm, but the features detected with this were not great (click on the second image to expand).
            Largely due to the use of the NIR image, it detected high intensity locations such as the shiny metal component of the headrest behind the face, and occasionally,
            eyes.
        </p>
        <p>
            Another change in procedure was made here to make use of the MediaPipe face mesh for good features to use for tracking optical flow. This allowed for just enough
            useful features to track facial components and get a good indicator of optical flow between frames, while also being easily run live. Additionally, since just the 
            face was being used for these landmarks, extraneous keypoints from the surroundings that may still appear in the face's bounding box, would not appear. The transition
            state is set based on the average flow in the horizontal and vertical decision, with the horizontal flow taking priority.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/op_flow/op_flow_farneback_frame_46_frame_47.png" alt="Farneback">
                <figcaption>Dense Optical Flow (Farneback)</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/op_flow/op_flow_lkt_frame_46_frame_47.png" alt="Lucas-Kanade (ST)">
                <figcaption>Sparse Optical Flow (Lucas-Kanade) with Shi-Tomasi Corner Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/op_flow/op_flow_lkt-mp_frame_46_frame_47.png" alt="Lucas-Kanade (MP)">
                <figcaption>Sparse Optical Flow (Lucas-Kanade) with MediaPipe Facial Landmarks</figcaption>
            </figure>
        </div>

        <h4>Decision Assignment</h4>
        <ul>
            <li>Calculate eye aspect ratio (EAR) from eye landmarks to determine whether eyes are open or closed.</li>
            <li>Estimate yaw and pitch from nose and eye corner positions to determine head orientation (straight, left, right, down).</li>
            <li>Combine optical flow transitions, EAR values, and head orientation to assign a decision for a frame based on thresholds for each of these.</li>
            <li>Assign final states:
                <ul>
                    <li>good: Eyes are open (or mid-blink) and head is facing straight. Not actively transitioning.</li>
                    <li>careful: Transitory state indicating that driver is not distracted, but appear to be transitioning head state.</li>
                    <li>distracted: Eyes are not open and/or head is not straight.</li>
                </ul>
            </li>
        </ul>
        <br>
        <p>
            This step in the pipeline is fairly straightforward, and largely consists of thresholding EAR values, yaw and pitch values, and making a decision tree to
            determine the final state of the driver. Eye aspect ratio is a formula that was proposed to detect eye closures [3]:
            $$\mathrm{EAR} = \frac{\lVert \mathbf{p}_2 - \mathbf{p}_6 \rVert + \lVert \mathbf{p}_3 - \mathbf{p}_5 \rVert}{2 \lVert \mathbf{p}_1 - \mathbf{p}_4 \rVert}$$
            This allowed us to use an effective and proven method of understanding when the user is closing their eyes on each frame. The yaw and pitch values were derived by
            using the midpoint between the eyes and the nose to determine head orientation. Through trial and error, effective thresholds were found with a light smoothing 
            applied to account for blinking and slight movements that do not fully cause a driver to be distracted, but if maintained for a lenghty amount of time, 
            could represent a distracted driver.
        </p>
    </section>

    <section class="section" id="experiments">
        <h2>Experiments and Results</h2>
        <h3>Dataset</h3>
        <p>
            For this project we will use the <a href="https://www.kaggle.com/datasets/zeyad1mashhour/driver-inattention-detection-dataset" target="_blank" rel="noopener noreferrer">Driver Inattention Detection Dataset</a> from Kaggle, 
            containing nearly 15,000 NIR images of drivers in 6 different states:
        </p>
        <ul>
            <li>Safe Driving</li>
            <li>Distracted Driving</li>
            <li>Dangerous Driving</li>
            <li>Sleepy Driving</li>
            <li>Drinking</li>
            <li>Yawn</li>
        </ul>
        <p>This dataset is split into three different sets: training, validation, and test. Each set contains the following quantity of test points:</p>
        <ul>
            <li>Training Set: 11,942 images</li>
            <li>Validation Set: 1,922 images</li>
            <li>Test Set: 985 images</li>
        </ul>
        <p>Below are sample frames extracted from the dataset.</p>
        <div class="image-grid">
            <figure>
                <img src="../proposal/static/man_focused.jpg" alt="Safe Driving">
                <figcaption>Safe Driving</figcaption>
            </figure>
            <figure>
                <img src="../proposal/static/man_drinking.jpg" alt="Drinking">
                <figcaption>Drinking</figcaption>
            </figure>
            <figure>
                <img src="../proposal/static/man_on_phone.jpg" alt="Distracted Driving">
                <figcaption>Distracted Driving</figcaption>
            </figure>
            <figure>
                <img src="../proposal/static/man_sleeping.jpg" alt="Sleepy Driving">
                <figcaption>Sleepy Driving</figcaption>
            </figure>
        </div>
        <h3>Experiments</h3>
        <p>
            Our ultimate goal will be real-time identification of dangerous driving behaviors, and we will attempt accomplishing this in three different ways:
        </p>
        <ul>
            <li>Deep learning using a pre-weighted model (i.e. resnet18)</li>
            <li>Deep learning using a custom-built CNN</li>
            <li>Optical flow analysis</li>
        </ul>
        <h3>Deep Learning Experiments</h3>
            <p>
                We trained multiple deep learning models and variants on this dataset, exploring
                different preprocessing pipelines, data balancing strategies, and optimizers.
                The two main architectures are the custom CNN and a ResNet-50 model fine-tuned
                with transfer learning.
            </p>

            <h4>Custom CNN Training Variants</h4>
            <p>
                For the custom CNN, we experimented with how data augmentation and class balancing
                affect convergence and generalization. In particular, we varied:
            </p>
            <ul>
                <li>Whether to use a class-balanced <code>WeightedRandomSampler</code> with replacement;</li>
                <li>Whether to include Gaussian blur in the augmentation pipeline.</li>
            </ul>
            <p>
                The plots below show training and validation accuracy/loss curves for three key
                configurations of the custom CNN:
            </p>

            <div class="image-grid">
                <figure>
                    <img src="./static/Deep_Learning/Plots/90% no color aug guassian weighted random sampler.png"
                        alt="Custom CNN with WeightedRandomSampler + Gaussian Blur">
                    <figcaption>Custom CNN – class-balanced sampler + Gaussian blur (~90% validation accuracy).</figcaption>
                </figure>
                <figure>
                    <img src="./static/Deep_Learning/Plots/90% no color aug.png"
                        alt="Custom CNN without Gaussian Blur">
                    <figcaption>Custom CNN – same setup but without Gaussian blur (~90% validation accuracy).</figcaption>
                </figure>
                <figure>
                    <img src="./static/Deep_Learning/Plots/92_7% no color guassian blur.png"
                        alt="Custom CNN with Gaussian Blur, best run">
                    <figcaption>Custom CNN – final configuration with Gaussian blur (~92.7% validation accuracy).</figcaption>
                </figure>
            </div>
            <p>
                Initially, we attempted to use a class-balanced <code>WeightedRandomSampler</code> with replacement,
                under the assumption that the dataset was significantly imbalanced. While the class distribution
                is not perfectly uniform, it is not severely imbalanced in the sense seen in real-world datasets
                where one or more classes may contain only a very small number of samples.
            </p>
            <p>
                In practice, the weighted sampling strategy duplicated images from the less frequent classes,
                which increased redundancy in the training data and ultimately hurt convergence rather than
                improving it. As a result, the model trained with the weighted sampler did not generalize as well
                as expected.
            </p>
            <p>
                The remaining two configurations were therefore trained without a weighted random sampler and
                instead relied on the natural class distribution of the dataset. Between these two runs, the key
                difference was the use of Gaussian blur as a data augmentation. The model trained without Gaussian
                blur served as a baseline, while the inclusion of Gaussian blur improved robustness to mild image
                noise and variations in focus. This final configuration achieved the best validation performance
                and the smallest gap between training and validation curves.
            </p>

            <h4>Batch Size Selection</h4>
            <p>
                We also tuned the batch size based on both GPU utilization and end-to-end epoch time.
                Using NVIDIA GPU monitoring tools during training, we measured how long one epoch took
                for different batch sizes:
            </p>
            <ul>
                <li>Batch size 8: ~276.27 seconds per epoch</li>
                <li>Batch size 16: ~248.52 seconds per epoch</li>
                <li>Batch size 32: ~467.30 seconds per epoch</li>
            </ul>
            <p>
                A batch size of 16 achieved the best trade-off: it saturated the GPU reasonably well
                while keeping each epoch short. Smaller batches under-utilized the GPU, and larger
                batches actually increased epoch time due to memory pressure and less efficient
                scheduling. For all subsequent experiments, we used <strong>batch size 16</strong>.
            </p>

            <h4>ResNet-50 Transfer Learning: SGD vs Adam</h4>
            <p>
                For the transfer-learning model (Modified ResNet-50), we compared stochastic gradient
                descent (SGD) with momentum against the Adam optimizer. In both cases, all layers up to
                <code>layer3</code> were frozen, and training was restricted to <code>layer4</code> and
                the final fully connected layer.
            </p>

            <div class="image-grid">
                <figure>
                    <img src="./static/Deep_Learning/Plots/91_7 resnet SGD.png"
                        alt="ResNet-50 training curves with SGD">
                    <figcaption>ResNet-50 – SGD (~91.7% validation accuracy, slow convergence).</figcaption>
                </figure>
                <figure>
                    <img src="./static/Deep_Learning/Plots/96% resnet adam.png"
                        alt="ResNet-50 training curves with Adam">
                    <figcaption>ResNet-50 – Adam (~96% validation accuracy, fast convergence).</figcaption>
                </figure>
            </div>

            <p>
                With SGD, validation accuracy starts relatively low and increases slowly over many epochs.
                In contrast, Adam reaches over 90% validation accuracy within just two epochs and continues
                to improve, ultimately achieving around 96% validation accuracy. This behavior is consistent
                with the benefits of Adam in transfer-learning scenarios: it adapts individual learning rates
                per parameter, which helps quickly fine-tune the small subset of trainable layers
                (<code>layer4</code> and <code>fc</code>) while keeping the pretrained weights stable.
            </p>
            <p>
                Because Adam finds a good region of the loss landscape much faster and yields higher final
                validation accuracy, we chose <strong>Adam</strong> as the optimizer for the final
                ResNet-50 model.
            </p>
        <h4>Landmarks and Optical Flow Analysis</h4>
        <p>
            This experiment will focus on using optical flow to detect motion-based indicators of distracted driving, such as hand movement, head turning, and signs of fatigue. Unlike the deep learning methods, this approach does not rely on labeled image classification but instead uses frame-to-frame motion estimation to detect behavior.
            We will simulate these behaviors while parked and log the system's ability to detect them accurately. Success will be measured by the system's responsiveness and its ability to correctly identify motion-based behaviors without excessive false positives. This method will also be evaluated for its feasibility on the NVIDIA Jetson Nano in terms of real-time performance and resource usage.
        </p>
        </p>
        <h4>Final Assessment</h4>
        <p>
            A comparison of these three methods will be performed at the conclusion of the implementations of each of them and be assessed based on the reliability of each method in 
            a real-time usecase, as well as the accuracy of each of these methods. Comparing deep learning methods with the optical flow analysis method can allow for a greater understanding of the strengths and weakness
            of each of these methods in the context of overall accuracy, as well as performance in a real-time scenario. When performing real-time testing, we will set up the camera and NVIDIA Jetson Nano system in our own cars 
            and evaluate the number of behaviors that are able to be picked up by each method.
        </p>
    </section>

    <section class="section" id="qual-results">
        <h2>Qualitative Results</h2>
        <h3>Deep Learning</h3>
        <h3>Landmarks and Optical Flow Analysis</h3>
        <p>
            As discussed in the previous section this process includes three steps in the pipeline. In the successful "distracted" classification that follows, note that
            the optical flows inform the system of movement that could contribute to being the movements being a distraction. It is the relation between the position of
            the nose and the midpoint between the eyes that determines the actual direction of the head (left, right, up, down).
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/distracted/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/distracted/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/distracted/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/distracted/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            Similarly, a successful "good" classification is as follows. Here, we see almost no optical flow, indicating no movement anywhere. Since eyes are open and
            the driver is facing straight with acceptable pitch and yaw, this is determined to include no distracted driving.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/good/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            While eyes are a major part of determining whether a driver is distracted (i.e sleepy), the pipeline accounts for blinks by allowing 9 frames before eyes 
            are considered closed, and possibly indicating that the driver is falling asleep.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/good_blink/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good_blink/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good_blink/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good_blink/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            Besides the eyes, temporal smoothing is also applied when making the decision that a frame indicates distracted driving. In the sequence below, 
            the optical flow suggests an impending turn to the left, but this could be a slight movement that is not necessarily unsafe. However,
            if the driver is in this state for more than 30 frames, it is formally classified as being "distracted" driving. In this example, this was only
            a slight movement and the status went back to "good" after.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/careful/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/careful/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/careful/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/careful/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            The full video that these frames have come from can be viewed here:
        </p>
        <div class="video-container">
            <video controls>
                <source src="static/optical_flow/final_web.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </section>

    <section class="section" id="conclusion">
        <h2>Conclusion</h2>
    </section>
    
    <section class="section" id="references">
        <h2>References</h2>
        <ol class="ieee-references">
            <li>
                Centers for Disease Control and Prevention, “Distracted Driving,” *CDC*, 2023. [Online]. Available: 
                <a href="https://www.cdc.gov/distracted-driving/about/index.html" target="_blank" rel="noopener noreferrer">
                    https://www.cdc.gov/distracted-driving/about/index.html
                </a>
            </li>
            <li>
                Insurify, “Texting and Driving Statistics,” *Insurify*, 2023. [Online]. Available: 
                <a href="https://insurify.com/car-insurance/insights/texting-and-driving-statistics/" target="_blank" rel="noopener noreferrer">
                    https://insurify.com/car-insurance/insights/texting-and-driving-statistics/
                </a>
            </li>
            <li>
                T. Soukupová and J. Čech, "Real-Time Eye Blink Detection using Facial Landmarks," 
                Center for Machine Perception, Department of Cybernetics, Faculty of Electrical Engineering, 
                Czech Technical University in Prague, Research Report No. 2016-05, 2016. [Online]. Available: 
                <a href="https://cmp.felk.cvut.cz/ftp/articles/cech/Soukupova-TR-2016-05.pdf" target="_blank" rel="noopener noreferrer">
                    https://cmp.felk.cvut.cz/ftp/articles/cech/Soukupova-TR-2016-05.pdf
                </a>
            </li>
        </ol>
    </section>
    <script>
        const images = document.querySelectorAll(".image-grid img");
        const overlay = document.querySelector(".overlay");

        images.forEach(img => {
            img.addEventListener("click", () => {
            if (img.classList.contains("expanded")) {
                img.classList.remove("expanded");
                overlay.style.display = "none";
            } else {
                images.forEach(i => i.classList.remove("expanded"));
                img.classList.add("expanded");
                overlay.style.display = "block";
            }
            });
        });

        overlay.addEventListener("click", () => {
            images.forEach(i => i.classList.remove("expanded"));
            overlay.style.display = "none";
        });
        </script>
</body>
</html>
