<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Final Report</title>
    <link rel="stylesheet" href="../css/proposal.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <div class="overlay"></div>
    <nav class="side-nav">
        <ul>
            <li><a href="#title">Top</a></li>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#approach">Approach</a></li>
            <li><a href="#experiments">Experiments and Results</a></li>
            <li><a href="#qual-results">Qualitative Results</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </nav>

    <section class="title" id="title">
        <h1>Final Report: Detecting Distracted Driving Using Computer Vision Techniques</h1>
        <div class="subtitles">
            <p>Aziz Shaik and Rayden Dodd</p>
            <p>ECE 5554: Computer Vision (Fall 2025)</p>
            <p>Virginia Tech</p>
        </div>
    </section>

    <section>
        <!-- Replace this image with something visual from this project -->
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/final2221.png" alt="Distracted Driving" style="width:300px; height:auto;">
                <figcaption>A Distracted Driver on the Phone</figcaption>
            </figure>
            <figure>
                <img src="./static/deep_learning/final2221.png" alt="Safe Driving" style="width:300px; height:auto;">
                <figcaption>A Safe Driver</figcaption>
            </figure>
        </div>
    </section>

    <section class="section" id="abstract">
        <h2>Abstract</h2>
        <p>
            Modern vehicles come equipped with a number of sensors for safe automotive operation, but these measures tend to be reactive
            to driver behavior rather than proactive. Using computer vision techniques, such as CNNs and optical flow methods with landmarks, we have
            developed two different approaches to capturing and identifying unsafe driver behavior for proactive in-vehicle safety
            interventions. Using deep learning methods, we obtained a maximum of 96% test accuracy with a RESNET based classifier and
            92.7% with a custom CNN. While specific accuracy statistics were not feasible due to the qualitative method of the landmark and optical
            flow method, it performed fairly well in a live environment, but did require a lot of threshold tuning which was sometimes inconsistent.
        </p>
    </section>

    <section class="section" id="Introduction">
        <h2>Introduction</h2>
        <p>
            Distracted driving is a leading cause of accidents around the world, posing a serious threat to public safety.
            According to the Centers for Disease Control and Prevention (CDC), over 3,100 individuals were killed and 
            approximately 424,000 were injured in motor vehicle crashes involving a distracted driver in the United States in 2019 [1]. 
            Equating to almost nine deaths per day, there is a very present need to solve this problem.
            Despite this, studies indicate that 68% of Gen Z drivers admit to using mobile devices while operating a vehicle [2], 
            highlighting a disconnect between the awareness of the risk and the actual driving behavior.
        </p>
        <p>
            Many modern vehicles incorporate systems to ensure drivers are focused by limiting interface access or flashing a warning when
            it detects irregular driving. However, these systems primarily respond to interactions with the vehicle itself and often 
            fail to capture the broader spectrum of risky driving behaviors.
        </p>
        <p>
            The core challenge presented by this problem lies in accurately identifying specific driver behaviors in real time using visual 
            data, with operator actions such as phone usage, eating, or signs of tiredness, and making intelligent decisions based on the 
            severity of the behavior. A system capable of detecting these patterns could help deter dangerous conduct and contribute to a 
            safer travel experience for passengers and nearby drivers.
        </p>
    </section>

    <section class="section" id="approach">
        <h2>Approach</h2>
        <p>
            For this problem, we attempt two different technical approaches: one method using deep learning model to classify behavior and another method making use
            of landmarks and optical flow analysis to characterize behavior. The approach differs for each with different success criteria for each method.
        </p>
        <h3>Deep Learning</h3>
        <p>
            The deep learning approach involved training two different convolutional neural networks (CNNs)
            to classify images of drivers into 6 different categories based on their behavior.
            This method leverages the power of deep learning to automatically learn features from the images
            that are indicative of distracted driving.
        </p>

        <h4>Preprocessing</h4>
        <p>
            All images are first converted from raw pixel data into PyTorch tensors and resized to a consistent
            resolution of <code>224 × 224</code> pixels. This unified resolution makes the data compatible with
            both the custom CNN and the ResNet-based model.
        </p>
        <p>
            To improve generalization, the training set is augmented with random transformations such as:
        </p>
        <ul>
            <li>Random horizontal flips</li>
            <li>Random rotations</li>
            <li>Color jitter (brightness and contrast)</li>
            <li>Gaussian blur</li>
        </ul>

        <h5>Custom CNN Preprocessing</h5>
        <p>
            For the custom CNN, the dataset statistics (mean and standard deviation) are computed directly from
            the training images using <code>get_image_means_stds</code>. These values are then used to normalize
            each input image, which helps stabilize training by putting all channels on a similar scale.
        </p>

        <h5>ResNet-50 Preprocessing</h5>
        <p>
            For the transfer learning setup with ResNet-50, the images are normalized using the standard
            ImageNet statistics (<code>mean = [0.485, 0.456, 0.406]</code>,
            <code>std = [0.229, 0.224, 0.225]</code>). This matches the preprocessing used when ResNet-50 was
            originally trained on ImageNet, allowing the pretrained weights to transfer effectively to the
            distracted driving task.
        </p>

        <h4>Model Architectures</h4>

        <h5>Custom CNN</h5>
        <p>
            The custom CNN is a relatively lightweight architecture built from four convolutional layers with
            batch normalization and max pooling, followed by an adaptive average pooling layer and three fully
            connected layers. The convolutional layers extract increasingly abstract spatial features from the
            input images, and the fully connected layers map these features to the 6 driver behavior classes.
        </p>
        <div class="cnn-architecture">
            <table>
                <thead>
                    <tr>
                        <th>Layer (type)</th>
                        <th>Output Shape</th>
                        <th>Param #</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Conv2d-1</td>
                        <td>[-1, 32, 224, 224]</td>
                        <td>896</td>
                    </tr>
                    <tr>
                        <td>BatchNorm2d-2</td>
                        <td>[-1, 32, 224, 224]</td>
                        <td>64</td>
                    </tr>
                    <tr>
                        <td>Conv2d-3</td>
                        <td>[-1, 64, 224, 224]</td>
                        <td>18,496</td>
                    </tr>
                    <tr>
                        <td>BatchNorm2d-4</td>
                        <td>[-1, 64, 224, 224]</td>
                        <td>128</td>
                    </tr>
                    <tr>
                        <td>MaxPool2d-5</td>
                        <td>[-1, 64, 112, 112]</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>Conv2d-6</td>
                        <td>[-1, 128, 112, 112]</td>
                        <td>73,856</td>
                    </tr>
                    <tr>
                        <td>BatchNorm2d-7</td>
                        <td>[-1, 128, 112, 112]</td>
                        <td>256</td>
                    </tr>
                    <tr>
                        <td>Conv2d-8</td>
                        <td>[-1, 128, 112, 112]</td>
                        <td>147,584</td>
                    </tr>
                    <tr>
                        <td>BatchNorm2d-9</td>
                        <td>[-1, 128, 112, 112]</td>
                        <td>256</td>
                    </tr>
                    <tr>
                        <td>MaxPool2d-10</td>
                        <td>[-1, 128, 56, 56]</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>AdaptiveAvgPool2d-11</td>
                        <td>[-1, 128, 14, 14]</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>Linear-12</td>
                        <td>[-1, 512]</td>
                        <td>12,845,568</td>
                    </tr>
                    <tr>
                        <td>Linear-13</td>
                        <td>[-1, 128]</td>
                        <td>65,664</td>
                    </tr>
                    <tr>
                        <td>Linear-14</td>
                        <td>[-1, 6]</td>
                        <td>774</td>
                    </tr>
                </tbody>
            </table>
            <p><strong>Total params:</strong> 13,153,542</p>
            <p><strong>Trainable params:</strong> 13,153,542</p>
            <p><strong>Non-trainable params:</strong> 0</p>
            <p><strong>Input size (MB):</strong> 0.57</p>
            <p><strong>Forward/backward pass size (MB):</strong> 131.88</p>
            <p><strong>Params size (MB):</strong> 50.18</p>
            <p><strong>Estimated total size (MB):</strong> 182.63</p>
        </div>

        <h5>ResNet-50 (Transfer Learning)</h5>
        <p>
            The second model is based on a ResNet-50 architecture pretrained on ImageNet. The original
            final fully connected (FC) layer is replaced with a new <code>nn.Linear</code> layer that
            outputs 6 logits, one for each driver behavior class. To adapt the model to this task while
            preserving most of the pretrained features, only the deepest residual block (<code>layer4</code>)
            and the final FC layer are allowed to update during training; all earlier layers are frozen.
        </p>
        <p>
            This fine-tuning strategy allows ResNet-50 to reuse its low-level and mid-level features
            (edges, textures, shapes) from ImageNet, while the high-level features in <code>layer4</code>
            and the classifier are updated to specialize in distracted driving detection.
        </p>

        <div class="cnn-architecture">
            <table>
                <thead>
                    <tr>
                        <th>Layer (type)</th>
                        <th>Output Shape</th>
                        <th>Param #</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Conv2d-1</td><td>[-1, 64, 112, 112]</td><td>9,408</td></tr>
                    <tr><td>BatchNorm2d-2</td><td>[-1, 64, 112, 112]</td><td>128</td></tr>
                    <tr><td>ReLU-3</td><td>[-1, 64, 112, 112]</td><td>0</td></tr>
                    <tr><td>MaxPool2d-4</td><td>[-1, 64, 56, 56]</td><td>0</td></tr>
                    <tr><td>...</td><td>...</td><td>...</td></tr>
                    <tr><td>AdaptiveAvgPool2d-173</td><td>[-1, 2048, 1, 1]</td><td>0</td></tr>
                    <tr><td>Linear-174</td><td>[-1, 6]</td><td>12,294</td></tr>
                    <tr><td>ResNet-175</td><td>[-1, 6]</td><td>0</td></tr>
                </tbody>
            </table>
            <p><strong>Total params:</strong> 23,520,326</p>
            <p><strong>Trainable params:</strong> 14,977,030</p>
            <p><strong>Non-trainable params:</strong> 8,543,296</p>
            <p><strong>Input size (MB):</strong> 0.57</p>
            <p><strong>Forward/backward pass size (MB):</strong> 286.55</p>
            <p><strong>Params size (MB):</strong> 89.72</p>
            <p><strong>Estimated total size (MB):</strong> 376.85</p>
        </div>

        <h4>Training</h4>

        <h5>Custom CNN Training</h5>
        <p>
            The custom CNN is trained from scratch using mini-batches of size <code>16</code> for
            <code>20</code> epochs. At each iteration, a batch of normalized images is passed through
            the network, the cross-entropy loss (with <code>label_smoothing = 0.05</code>) is computed,
            and the gradients are backpropagated. The optimizer is stochastic gradient descent (SGD)
            with a learning rate of <code>0.001</code>, momentum <code>0.9</code>, and weight decay
            <code>1e-4</code>, which helps the model converge while reducing overfitting.
        </p>
        <p>
            Data loading is parallelized with <code>num_workers = 4</code>, and each epoch corresponds
            to a full pass through the training set. Training and validation losses and accuracies are
            recorded so that the learning curves can be visualized after training.
        </p>

        <h5>ResNet-50 Training</h5>
        <p>
            The modified ResNet-50 model is trained using transfer learning. The network is initialized
            with ImageNet-pretrained weights, and only the parameters in <code>layer4</code> and the
            final <code>fc</code> layer are updated. Training also runs for <code>20</code> epochs with
            mini-batches of size <code>16</code>, but the optimization setup differs from the custom CNN.
        </p>
        <p>
            An Adam optimizer is used with separate learning rates: a slightly higher rate for the new
            fully connected layer and a smaller rate for <code>layer4</code>. This allows the new
            classifier to adapt quickly while making more conservative updates to the pretrained
            convolutional features. A smaller amount of label smoothing is applied than in the custom CNN,
            reflecting the fact that ResNet-50 starts from a strong pretrained representation.
        </p>

        <h4>Evaluation</h4>
        <p>
            After each training epoch, both models are evaluated on a separate validation set without
            updating any weights. The models are switched to evaluation mode, and metrics such as
            accuracy, precision, recall, and F1-score (macro-averaged across the 6 classes) are computed,
            along with the validation loss.
        </p>
        <p>
            These validation metrics are used to monitor for overfitting: if training accuracy continues
            to improve while validation accuracy or F1-score stagnates or decreases, the model is likely
            memorizing the training data. The model checkpoint that achieves the best validation
            accuracy is saved to disk and treated as the “best” version for later testing.
        </p>

        <h4>Testing</h4>
        <p>
            Once training is complete, the best-performing checkpoint for each model (based on validation
            accuracy) is evaluated on a held-out test set that was never used during training or validation.
            This final evaluation provides an unbiased estimate of how well each model can classify driver
            behavior in real-world scenarios.
        </p>
        <p>
            The same metrics accuracy, precision, recall, and F1-score are reported on the test set,
            allowing a direct comparison between the performance of the custom CNN and the transfer-learning
            ResNet-50 model.
        </p>

        <h3>Landmarks and Optical Flow Analysis</h3>
        <p>
            As an alternative to a pure deep learning procedure, we implement a motion-based approach using optical flow to detect movements using
            facial landmarks. This method analyzes frame-to-frame changes in video captured from a live camera feed or video, 
            allowing us to track  head movement, eye movement, and other facial features without relying on labeled image classification.
        </p>
        <h4>Face Detection</h4>
        <ul>
            <li>Use MediaPipe's pre-trained facial detection model to efficiently locate bounding boxes around the driver's face in each frame</li>
            <li>If there are multiple faces found, select the largest one to filter out other faces and possible false detections.</li>
            <li>Capture the dimensions of this bounding box as the region of interest to work with.</li>
        </ul>
        <br>
        <p>
            Initially, the Haar cascade classifier was used for facial detection, and while it was very fast, it tended to be inaccurate and the bounding box
            that was returned was often to small and did not capture enough of the face. Additionally, there would be many frames where it simply did not capture
            any faces. From a sample of 4089 frames, the Haar cascade classifier could not locate a face in 1315 of these frames, while MediaPipe only missed 72 frames.
        </p>
            
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/facebox_frame_2911_haar.png" alt="Haar" style="width:400px; height:auto;">
                <figcaption>Bounding Box with Haar Cascades</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/facebox_frame_2911_mp.png" alt="MediaPipe" style="width:400px; height:auto;">
                <figcaption>Bounding Box with MediaPipe</figcaption>
            </figure>
        </div>

        <p>
            With the bounding box, we could ignore anything outside of it and use this to reduce computation and possible errors. If there are multiple people in the car,
            the main face in front of the camera, the driver, would be the only one with a facial mesh formed for it, further simplifying computation.
        </p>

        <h4>Optical Flow Analysis</h4>
        <ul>
            <li>Extract facial landmarks from consecutive frames using MediaPipe's face mesh.</li>
            <li>Apply Lucas-Kanade optical flow (<code>cv2.calcOpticalFlowPyrLK</code>) to track landmark displacement between frames.</li>
            <li>Compute average horizontal (dx) and vertical (dy) shifts to infer head movement.</li>
            <li>Threshold displacements to classify transitions in one of the following states:
                <ul>
                    <li>turning left</li>
                    <li>turning right</li>
                    <li>looking up</li>
                    <li>looking down</li>
                </ul>
            </li>
        </ul>
        <br>
        <p>
            The initial approach involved making use of Farneback optical flow, but even with the cropped ROI, this was far too computationally expensive. Running live
            was very slow and eventually crashed, and running with an MP4 file caused crashes as well, doing about 2000 frames in 20 minutes. For this reason, we switched
            to using sparse optical flow, the Lucas-Kanade algorithm, which ran much faster and was usable for live detection. The initial implementation made use of
            <code>cv2.goodFeaturesToTrack</code>, which uses the Shi-Tomasi corner detection algorithm, but the features detected with this were not great (click on the second image to expand).
            Largely due to the use of the NIR image, it detected high intensity locations such as the shiny metal component of the headrest behind the face, and occasionally,
            eyes.
        </p>
        <p>
            Another change in procedure was made here to make use of the MediaPipe face mesh for good features to use for tracking optical flow. This allowed for just enough
            useful features to track facial components and get a good indicator of optical flow between frames, while also being easily run live. Additionally, since just the 
            face was being used for these landmarks, extraneous keypoints from the surroundings that may still appear in the face's bounding box, would not appear. The transition
            state is set based on the average flow in the horizontal and vertical decision, with the horizontal flow taking priority.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/op_flow/op_flow_farneback_frame_46_frame_47.png" alt="Farneback">
                <figcaption>Dense Optical Flow (Farneback)</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/op_flow/op_flow_lkt_frame_46_frame_47.png" alt="Lucas-Kanade (ST)">
                <figcaption>Sparse Optical Flow (Lucas-Kanade) with Shi-Tomasi Corner Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/op_flow/op_flow_lkt-mp_frame_46_frame_47.png" alt="Lucas-Kanade (MP)">
                <figcaption>Sparse Optical Flow (Lucas-Kanade) with MediaPipe Facial Landmarks</figcaption>
            </figure>
        </div>

        <h4>Decision Assignment</h4>
        <ul>
            <li>Calculate eye aspect ratio (EAR) from eye landmarks to determine whether eyes are open or closed.</li>
            <li>Estimate yaw and pitch from nose and eye corner positions to determine head orientation (straight, left, right, down).</li>
            <li>Combine optical flow transitions, EAR values, and head orientation to assign a decision for a frame based on thresholds for each of these.</li>
            <li>Assign final states:
                <ul>
                    <li>good: Eyes are open (or mid-blink) and head is facing straight. Not actively transitioning.</li>
                    <li>careful: Transitory state indicating that driver is not distracted, but appear to be transitioning head state.</li>
                    <li>distracted: Eyes are not open and/or head is not straight.</li>
                </ul>
            </li>
        </ul>
        <br>
        <p>
            This step in the pipeline is fairly straightforward, and largely consists of thresholding EAR values, yaw and pitch values, and making a decision tree to
            determine the final state of the driver. Eye aspect ratio is a formula that was proposed to detect eye closures [3]:
            $$\mathrm{EAR} = \frac{\lVert \mathbf{p}_2 - \mathbf{p}_6 \rVert + \lVert \mathbf{p}_3 - \mathbf{p}_5 \rVert}{2 \lVert \mathbf{p}_1 - \mathbf{p}_4 \rVert}$$
            This allowed us to use an effective and proven method of understanding when the user is closing their eyes on each frame. The optical flow was used to find 
            transition states that cause a frame to be labelled "careful" if it indicated that the original state of the driver's head was facing straight. This allowed for 
            a smoother transition into determining when potentially "distracted" frames were coming up. The yaw and pitch values were derived by
            using the midpoint between the eyes and the nose to determine head orientation. Through trial and error, effective thresholds were found with a light smoothing 
            applied to account for blinking and slight movements that do not fully cause a driver to be distracted, but if maintained for a lenghty amount of time, 
            could represent a distracted driver. One thing to note here is that, unlike the CNNs, there were no specific labels, so it is a combination of these thresholds
            and decision tree to make the binary classification between distracted or not (the "careful" label is ultimately the same as "good").
        </p>
    </section>

    <section class="section" id="experiments">
        <h2>Experiments and Results</h2>
        <h3>Dataset</h3>
        <p>
            For this project we will use the <a href="https://www.kaggle.com/datasets/zeyad1mashhour/driver-inattention-detection-dataset" target="_blank" rel="noopener noreferrer">Driver Inattention Detection Dataset</a> from Kaggle, 
            containing nearly 15,000 NIR images of drivers in 6 different states:
        </p>
        <ul>
            <li>Safe Driving</li>
            <li>Distracted Driving</li>
            <li>Dangerous Driving</li>
            <li>Sleepy Driving</li>
            <li>Drinking</li>
            <li>Yawn</li>
        </ul>
        <p>This dataset is split into three different sets: training, validation, and test. Each set contains the following quantity of test points:</p>
        <ul>
            <li>Training Set: 11,942 images</li>
            <li>Validation Set: 1,922 images</li>
            <li>Test Set: 985 images</li>
        </ul>
        <p>Below are sample frames extracted from the dataset.</p>
        <div class="image-grid">
            <figure>
                <img src="../proposal/static/man_focused.jpg" alt="Safe Driving">
                <figcaption>Safe Driving</figcaption>
            </figure>
            <figure>
                <img src="../proposal/static/man_drinking.jpg" alt="Drinking">
                <figcaption>Drinking</figcaption>
            </figure>
            <figure>
                <img src="../proposal/static/man_on_phone.jpg" alt="Distracted Driving">
                <figcaption>Distracted Driving</figcaption>
            </figure>
            <figure>
                <img src="../proposal/static/man_sleeping.jpg" alt="Sleepy Driving">
                <figcaption>Sleepy Driving</figcaption>
            </figure>
        </div>
        <h3>Experiments</h3>
        <p>
            We have completed three different experimental approach to achieve our overall goal of real-time identification of dangerous driving:
        </p>
        <ul>
            <li>Deep learning using a pre-weighted model (i.e. resnet18)</li>
            <li>Deep learning using a custom-built CNN</li>
            <li>Optical flow analysis</li>
        </ul>
        <h4>Deep Learning Experiments</h4>
        <p>
            We trained multiple deep learning models and variants on this dataset, exploring
            different preprocessing pipelines, data balancing strategies, and optimizers.
            The two main architectures are the custom CNN and a ResNet-50 model fine-tuned
            with transfer learning.
        </p>

        <h5>Custom CNN Training Variants</h5>
        <p>
            For the custom CNN, we experimented with how data augmentation and class balancing
            affect convergence and generalization. In particular, we varied:
        </p>
        <ul>
            <li>Whether to use a class-balanced <code>WeightedRandomSampler</code> with replacement;</li>
            <li>Whether to include Gaussian blur in the augmentation pipeline.</li>
        </ul>
        <br>
        <p>
            The plots below show training and validation accuracy/loss curves for three key
            configurations of the custom CNN:
        </p>

        <div class="image-grid">
            <figure>
                <img src="./static/Deep_Learning/Plots/90% no color aug guassian weighted random sampler.png"
                    alt="Custom CNN with WeightedRandomSampler + Gaussian Blur">
                <figcaption>Custom CNN: class-balanced sampler + Gaussian blur (~90% validation accuracy)</figcaption>
            </figure>
            <figure>
                <img src="./static/Deep_Learning/Plots/90% no color aug.png"
                    alt="Custom CNN without Gaussian Blur">
                <figcaption>Custom CNN: same setup but without Gaussian blur (~90% validation accuracy)</figcaption>
            </figure>
            <figure>
                <img src="./static/Deep_Learning/Plots/92_7% no color guassian blur.png"
                    alt="Custom CNN with Gaussian Blur, best run">
                <figcaption>Custom CNN: final configuration with Gaussian blur (~92.7% validation accuracy)</figcaption>
            </figure>
        </div>
        <p>
            Initially, we attempted to use a class-balanced <code>WeightedRandomSampler</code> with replacement,
            under the assumption that the dataset was significantly imbalanced. While the class distribution
            is not perfectly uniform, it is not severely imbalanced in the sense seen in real-world datasets
            where one or more classes may contain only a very small number of samples.
        </p>
        <p>
            In practice, the weighted sampling strategy duplicated images from the less frequent classes,
            which increased redundancy in the training data and ultimately hurt convergence rather than
            improving it. As a result, the model trained with the weighted sampler did not generalize as well
            as expected and did the worst on testing data compared to the others.
        </p>
        <p>
            The remaining two configurations were therefore trained without a weighted random sampler and
            instead relied on the natural class distribution of the dataset. Between these two runs, the key
            difference was the use of Gaussian blur as a data augmentation. The model trained without Gaussian
            blur served as a baseline, while the inclusion of Gaussian blur improved robustness to mild image
            noise and variations in focus. This final configuration achieved the best validation performance
            and the smallest gap between training and validation curves.
        </p>

        <h5>Batch Size Selection</h5>
        <p>
            We also tuned the batch size based on both GPU utilization and end-to-end epoch time.
            Using NVIDIA GPU monitoring tools during training, we measured how long one epoch took
            for different batch sizes:
        </p>
        <ul>
            <li>Batch size 8: ~276.27 seconds per epoch</li>
            <li>Batch size 16: ~248.52 seconds per epoch</li>
            <li>Batch size 32: ~467.30 seconds per epoch</li>
        </ul>
        <p>
            A batch size of 16 achieved the best trade-off: it saturated the GPU reasonably well
            while keeping each epoch short. Smaller batches under-utilized the GPU, and larger
            batches actually increased epoch time due to memory pressure and less efficient
            scheduling. For all subsequent experiments, we used <strong>batch size 16</strong>.
        </p>

        <h5>ResNet-50 Transfer Learning: SGD vs Adam</h5>
        <p>
            For the transfer-learning model (Modified ResNet-50), we compared stochastic gradient
            descent (SGD) with momentum against the Adam optimizer. In both cases, all layers up to
            <code>layer3</code> were frozen, and training was restricted to <code>layer4</code> and
            the final fully connected layer.
        </p>

        <div class="image-grid">
            <figure>
                <img src="./static/Deep_Learning/Plots/91_7 resnet SGD.png"
                    alt="ResNet-50 training curves with SGD">
                <figcaption>ResNet-50 with SGD (~91.7% validation accuracy, slow convergence)</figcaption>
            </figure>
            <figure>
                <img src="./static/Deep_Learning/Plots/96% resnet adam.png"
                    alt="ResNet-50 training curves with Adam">
                <figcaption>ResNet-50 with Adam (~96% validation accuracy, fast convergence)</figcaption>
            </figure>
        </div>

        <p>
            With SGD, validation accuracy starts relatively low and increases slowly over many epochs.
            In contrast, Adam reaches over 90% validation accuracy within just two epochs and continues
            to improve, ultimately achieving around 96% validation accuracy. This behavior is consistent
            with the benefits of Adam in transfer-learning scenarios: it adapts individual learning rates
            per parameter, which helps quickly fine-tune the small subset of trainable layers
            (<code>layer4</code> and <code>fc</code>) while keeping the pretrained weights stable.
        </p>
        <p>
            Because Adam finds a good region of the loss landscape much faster and yields higher final
            validation accuracy, we chose <strong>Adam</strong> as the optimizer for the final
            ResNet-50 model.
        </p>

        <h4>Deep Learning Results</h4>

        <h5>Custom CNN</h5>
        <p>
            The best performing custom CNN model was evaluated on the held-out test set.
            The model achieved an overall test accuracy of <strong>91.57%</strong>, which
            closely matches the validation accuracy observed during training. This close
            alignment between validation and test performance indicates good generalization
            and minimal overfitting.
        </p>

        <p>
            The table below summarizes the overall performance of the custom CNN on the test set:
        </p>

        <div class="cnn-architecture">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Accuracy</td><td>0.9157</td></tr>
                    <tr><td>Precision (Macro Avg)</td><td>0.8909</td></tr>
                    <tr><td>Recall (Macro Avg)</td><td>0.8914</td></tr>
                    <tr><td>F1-score (Macro Avg)</td><td>0.8894</td></tr>
                </tbody>
            </table>
        </div>

        <p>
            A per-class breakdown further illustrates how class distribution impacts performance:
        </p>

        <div class="cnn-architecture">
            <table>
                <thead>
                    <tr>
                        <th>Class</th>
                        <th>Precision</th>
                        <th>Recall</th>
                        <th>F1-score</th>
                        <th>Support</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Dangerous Driving</td><td>0.98</td><td>0.99</td><td>0.99</td><td>301</td></tr>
                    <tr><td>Distracted</td><td>0.84</td><td>0.88</td><td>0.86</td><td>152</td></tr>
                    <tr><td>Drinking</td><td>0.95</td><td>0.80</td><td>0.87</td><td>25</td></tr>
                    <tr><td>Safe Driving</td><td>0.92</td><td>0.90</td><td>0.91</td><td>412</td></tr>
                    <tr><td>Sleepy Driving</td><td>0.79</td><td>0.83</td><td>0.81</td><td>69</td></tr>
                    <tr><td>Yawn</td><td>0.86</td><td>0.96</td><td>0.91</td><td>26</td></tr>
                </tbody>
            </table>
        </div>

        <p>
            Classes with a large number of training samples, such as <em>Safe Driving</em> and
            <em>Dangerous Driving</em>, achieve high precision and recall. This indicates that
            the model reliably recognizes these behaviors and rarely confuses them with other classes.
        </p>
        <p>
            Minority classes show different behavior. The <em>Yawn</em>
            class exhibits high recall but lower precision. This means the model successfully
            detects most true yawning events but occasionally misclassifies other behaviors as yawns.
            In practice, this reflects a conservative detection strategy. The model is effective at
            catching fatigue-related cues, even if some false positives are introduced.
        </p>

        <h5>ResNet-50 (Transfer Learning)</h5>
        <p>
            The ResNet-50 transfer learning model outperformed the custom CNN across all evaluation
            metrics. It achieved a test accuracy of <strong>95.84%</strong>, once again closely matching
            the validation accuracy observed during training. This consistency confirms strong
            generalization to never before seen data.
        </p>

        <div class="cnn-architecture">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Accuracy</td><td>0.9584</td></tr>
                    <tr><td>Precision (Macro Avg)</td><td>0.9713</td></tr>
                    <tr><td>Recall (Macro Avg)</td><td>0.9462</td></tr>
                    <tr><td>F1-score (Macro Avg)</td><td>0.9578</td></tr>
                </tbody>
            </table>
        </div>

        <div class="cnn-architecture">
            <table>
                <thead>
                    <tr>
                        <th>Class</th>
                        <th>Precision</th>
                        <th>Recall</th>
                        <th>F1-score</th>
                        <th>Support</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Dangerous Driving</td><td>0.98</td><td>0.99</td><td>0.99</td><td>301</td></tr>
                    <tr><td>Distracted</td><td>0.97</td><td>0.84</td><td>0.90</td><td>152</td></tr>
                    <tr><td>Drinking</td><td>1.00</td><td>0.96</td><td>0.98</td><td>25</td></tr>
                    <tr><td>Safe Driving</td><td>0.94</td><td>0.98</td><td>0.96</td><td>412</td></tr>
                    <tr><td>Sleepy Driving</td><td>0.94</td><td>0.90</td><td>0.92</td><td>69</td></tr>
                    <tr><td>Yawn</td><td>1.00</td><td>1.00</td><td>1.00</td><td>26</td></tr>
                </tbody>
            </table>
        </div>

        <p>
            Compared to the custom CNN, ResNet-50 significantly improves precision and recall for
            underrepresented classes such as <em>Drinking</em>, <em>Sleepy Driving</em>, and
            <em>Yawn</em>. The <em>Yawn</em> class achieves perfect precision and recall,
            indicating that the model both detects yawning reliably and avoids false positives.
        </p>
        <p>
            These improvements are driven by transfer learning from ImageNet, which provides strong
            and generalizable visual features. Fine-tuning only the deepest layers helps ResNet-50
            to adapt effectively to subtle driver behaviors while maintaining robustness.
        </p>

        <h5>Deep Learning Real-World Testing</h5>
        <p>
        </p>

        <h4>Landmarks and Optical Flow Analysis</h4>
        <p>
            This approach was largely qualitative as it was not feasible to manually label each of the thousands of frames for analysis, but we had two main experimental setups for this approach.
            The first was live testing where the code is run and computations happen on the current frame, which performed quite well with no issues in video quality or lag.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/plots/fps_timeline.png" alt="FPS Timeline" style="width:700px; height:auto;">
                <figcaption>FPS Timeline with a Jetson Nano and 720p Webcam</figcaption>
            </figure>
        </div>
        <p>
            Live testing was conducted both on standard PCs and on a Jetson Nano running an Ubuntu image. In all cases, the video playback was smooth with no visible choppiness.
            As seen above, other than a few jumps, the FPS was consistently between 10 and 15 FPS largely as a consequence of how the FPS measurement was done. 
            To highlight the true performance, the smoothed FPS has also been plotted to show the consistent frame rate that this approach was able to achieve.
        </p>
        <p>
            The second experimental setup was using a pre-recorded MP4 file. This was the most used setup as it allowed for a consistent video that could be used to test our different
            approaches and better compare the outputs. Using the optical flow method the video that is in the <a href="#qual-results">Qualitative Results</a> section, we plotted
            the heatmaps seen below.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/plots/focus_state_comparisons.png" alt="Focus State Heatmps">
                <figcaption>Heatmaps of the Characterizing Components vs Focus State</figcaption>
            </figure>
        </div>
        <p>
            As expected, the most common focus states for the head were "good" and "distracted" with the "careful" state allowing for some nuance in the driver's actions. While the
            main strategy was if the head is facing straight and the eyes are open, the user is focused and not distracted, we can see that the user was still determined to be distracted
            if they were facing straight ahead due to eyes closing or lateral motion vectors causing the system to flag that frame as "careful" or possibly even "distracted" if the driver
            was in that position for multiple consecutive frames. Another interesting observation with these heatmaps is with "Transition vs Focus State" where we can see that most of the 
            distractions were made in the section with no transition, indicating that the user spent most of the final decisions were made when the user was not transitioning between head
            orientations
        </p>
        <p>
            As mentioned in the approach, thresholds formed the majority of tunable parameters for this model. While the MediaPipe face detection and the actual Lucas-Kanade optical flow
            functions had many parameters, we set these early on and did not have to vary them to enhance the functionality of the overall detector. The following are the thresholds that
            were manipulated as part of testing:
            <ul>
                <li>Lateral Flow</li>
                <li>Vertical Flow</li>
                <li>EAR</li>
                <li>Yaw</li>
                <li>Pitch</li>
                <li>Eye Closed Count</li>
                <li>Distracted Count</li>
            </ul>
        </p>
        <p>
            Manual testing was used to determine the optimal values for each of these thresholds, with EAR being the most straightforward to determine whether a driver's eyes were
            open or closed. Flow was also fairly straightforward, but did require refinement to ensure that movements that were a very slight adjustment were not captured, while also
            ensuring that larger movements were captured. The current values (of 3 for average lateral flow and 5 for average vertical flow) worked well and were assessed at the same
            time to output one directional label. The lateral directions were given priority as this was where most of the distractions happened, i.e. looking in the upper right would
            give a transition label of "turning right" over "looking up," but this set of labels could easily have been expanded to include more directions. In the images below,
            we see that a lower vertical flow threshold contributes to sensitive vertical transition decisions where visually the driver's head orientation did not seem to change, but
            the detector picks up on the slight downward flow.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/experiments/final_frame_805_3dy.png" alt="3 dy" style="width:400px; height:auto;">
                <figcaption>Test Threshold for Average Vertical Flow of 3</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/experiments/final_frame_805_5dy.png" alt="5 dy" style="width:400px; height:auto;">
                <figcaption>Used Threshold for Average Vertical Flow of 5</figcaption>
            </figure>
        </div>
        <p>
            The counts for eye closures and distractions were originally implemented to allow for the driver to turn left to check the mirrors or capture 
            blinks. If a driver exceeds 30 frames in this middle state, they are considered to be a "distracted" driver. Without these counts, the detector would be very quick to assign
            "detected" decisions, even if the user was simply checking their rear-view mirror.
            Yaw and pitch thresholds were more difficult to determine but the intention was to capture a range of yaw and pitch that would be considered "normal" movement that an 
            undistracted driver would do. This was challenging as different people had different ranges and it was heavily dependent on the camera position. When setting up the camera,
            we had to be sure that the driver was in a fixed position in front of the camera looking straight, otherwise they would appear as slightly leaning in one direction or another,
            causing a "distracted" decision. As seen below, the left image had a very slight left movement that caused the lower yaw threshold of 0.01 to trigger and mark the frame
            as potentially dangeorous, while the yaw threshold that is actually implemented maintained a "good" focus state throughout this.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/experiments/final_frame_1448_0.05yaw.png" alt="0.05 yaw" style="width:400px; height:auto;">
                <figcaption>Test 0.05 Yaw Threshold</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/experiments/final_frame_1448_0.1yaw.png" alt="0.1 yaw" style="width:400px; height:auto;">
                <figcaption>Used 0.1 Yaw Threshold</figcaption>
            </figure>
        </div>
        <p>
            Overall, this approach worked very well despite having to do a fair amount of tuning and testing of thresholds as it relied on facial position and eye closure state to
            determine the focus state of the driver. While there are still flaws with selecting the right thresholds, there is great potential to move forward with an approach 
            based on these methods, possibly implementing a live calibration phase before each use that acclimates the detector to what ranges can be expected for a particular
            user's focus states.
        </p>
        <h4>Final Assessment</h4>
        <p>
            Ultimately, the approach that made use of landmarks and optical flow was a far more reliable method, giving much better live results, as well as being suitable to any
            type of camera that is used. The deep learning approach returned excellent accuracy scores, but struggled on a new custom example, as well as when testing live.
        </p>
    </section>

    <section class="section" id="qual-results">
        <h2>Qualitative Results</h2>
        <h3>Deep Learning</h3>
        <p>
            The full video that these frames have come from can be viewed here:
        </p>
        <div class="video-container">
            <video controls>
                <source src="static/Deep_Learning/aziz_result1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <h3>Landmarks and Optical Flow Analysis</h3>
        <p>
            As discussed in the previous section this process includes three steps in the pipeline. In the successful "distracted" classification that follows, note that
            the optical flows inform the system of movement that could contribute to being the movements being a distraction. It is the relation between the position of
            the nose and the midpoint between the eyes that determines the actual direction of the head (left, right, up, down).
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/distracted/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/distracted/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/distracted/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/distracted/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            Similarly, a successful "good" classification is as follows. Here, we see almost no optical flow, indicating no movement anywhere. Since eyes are open and
            the driver is facing straight with acceptable pitch and yaw, this is determined to include no distracted driving.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/good/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            While eyes are a major part of determining whether a driver is distracted (i.e sleepy), the pipeline accounts for blinks by allowing 9 frames before eyes 
            are considered closed, and possibly indicating that the driver is falling asleep.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/good_blink/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good_blink/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good_blink/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good_blink/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            Besides the eyes, temporal smoothing is also applied when making the decision that a frame indicates distracted driving. In the sequence below, 
            the optical flow suggests an impending turn to the left, but this could be a slight movement that is not necessarily unsafe. However,
            if the driver is in this state for more than 30 frames, it is formally classified as being "distracted" driving. In this example, this was only
            a slight movement and the status went back to "good" after.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/careful/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/careful/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/careful/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/careful/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            The full video that these frames have come from can be viewed here:
        </p>
        <div class="video-container">
            <video controls>
                <source src="static/optical_flow/final_web.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </section>

    <section class="section" id="conclusion">
        <h2>Conclusion</h2>
    </section>
    
    <section class="section" id="references">
        <h2>References</h2>
        <ol class="ieee-references">
            <li>
                Centers for Disease Control and Prevention, “Distracted Driving,” *CDC*, 2023. [Online]. Available: 
                <a href="https://www.cdc.gov/distracted-driving/about/index.html" target="_blank" rel="noopener noreferrer">
                    https://www.cdc.gov/distracted-driving/about/index.html
                </a>
            </li>
            <li>
                Insurify, “Texting and Driving Statistics,” *Insurify*, 2023. [Online]. Available: 
                <a href="https://insurify.com/car-insurance/insights/texting-and-driving-statistics/" target="_blank" rel="noopener noreferrer">
                    https://insurify.com/car-insurance/insights/texting-and-driving-statistics/
                </a>
            </li>
            <li>
                T. Soukupová and J. Čech, "Real-Time Eye Blink Detection using Facial Landmarks," 
                Center for Machine Perception, Department of Cybernetics, Faculty of Electrical Engineering, 
                Czech Technical University in Prague, Research Report No. 2016-05, 2016. [Online]. Available: 
                <a href="https://cmp.felk.cvut.cz/ftp/articles/cech/Soukupova-TR-2016-05.pdf" target="_blank" rel="noopener noreferrer">
                    https://cmp.felk.cvut.cz/ftp/articles/cech/Soukupova-TR-2016-05.pdf
                </a>
            </li>
        </ol>
    </section>
    <script>
        const images = document.querySelectorAll(".image-grid img");
        const overlay = document.querySelector(".overlay");

        images.forEach(img => {
            img.addEventListener("click", () => {
            if (img.classList.contains("expanded")) {
                img.classList.remove("expanded");
                overlay.style.display = "none";
            } else {
                images.forEach(i => i.classList.remove("expanded"));
                img.classList.add("expanded");
                overlay.style.display = "block";
            }
            });
        });

        overlay.addEventListener("click", () => {
            images.forEach(i => i.classList.remove("expanded"));
            overlay.style.display = "none";
        });
        </script>
</body>
</html>
