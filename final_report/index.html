<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Final Report</title>
    <link rel="stylesheet" href="../css/proposal.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <div class="overlay"></div>
    <nav class="side-nav">
        <ul>
            <li><a href="#title">Top</a></li>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#approach">Approach</a></li>
            <li><a href="#experiments">Experiments and Results</a></li>
            <li><a href="#qual-results">Qualitative Results</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </nav>

    <section class="title" id="title">
        <h1>Final Report: Detecting Distracted Driving Using Computer Vision Techniques</h1>
        <div class="subtitles">
            <p>Aziz Shaik and Rayden Dodd</p>
            <p>ECE 5554: Computer Vision (Fall 2025)</p>
            <p>Virginia Tech</p>
        </div>
    </section>

    <section>
        <!-- Replace this image with something visual from this project -->
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/final2221.png" alt="Distracted Driving" style="width:300px; height:auto;">
                <figcaption>A Distracted Driver on the Phone</figcaption>
            </figure>
            <figure>
                <img src="./static/deep_learning/final2221.png" alt="Safe Driving" style="width:300px; height:auto;">
                <figcaption>A Safe Driver</figcaption>
            </figure>
        </div>
    </section>

    <section class="section" id="abstract">
        <h2>Abstract</h2>
        <p>
            Modern vehicles come equipped with a number of sensors for safe automotive operation, but these measures tend to be reactive
            to driver behavior rather than proactive. Using computer vision techniques, such as CNNs and optical flow methods with landmarks, we have
            developed two different approaches to capturing and identifying unsafe driver behavior for proactive in-vehicle safety
            interventions. Using deep learning methods, we obtained a maximum of 96% test accuracy with a RESNET based classifier and
            92.7% with a custom CNN. While specific accuracy statistics were not feasible due to the qualitative method of the landmark and optical
            flow method, it performed reasonably well in a live environment, but did require a lot of threshold tuning which was sometimes inconsistent.
        </p>
    </section>

    <section class="section" id="Introduction">
        <h2>Introduction</h2>
        <p>
            Distracted driving is a leading cause of accidents around the world, posing a serious threat to public safety.
            According to the Centers for Disease Control and Prevention (CDC), over 3,100 individuals were killed and 
            approximately 424,000 were injured in motor vehicle crashes involving a distracted driver in the United States in 2019 [1]. 
            Equating to almost nine deaths per day, there is a very present need to solve this problem.
            Despite this, studies indicate that 68% of Gen Z drivers admit to using mobile devices while operating a vehicle [2], 
            highlighting a disconnect between the awareness of the risk and the actual driving behavior.
        </p>
        <p>
            Many modern vehicles incorporate systems to ensure drivers are focused by limiting interface access or flashing a warning when
            it detects irregular driving. However, these systems primarily respond to interactions with the vehicle itself and often 
            fail to capture the broader spectrum of risky driving behaviors.
        </p>
        <p>
            The core challenge presented by this problem lies in accurately identifying specific driver behaviors in real time using visual 
            data, with operator actions such as phone usage, eating, or signs of tiredness, and making intelligent decisions based on the 
            severity of the behavior. A system capable of detecting these patterns could help deter dangerous conduct and contribute to a 
            safer travel experience for passengers and nearby drivers.
        </p>
    </section>

    <section class="section" id="approach">
        <h2>Approach</h2>
        <p>
            For this problem, we attempt two different technical approaches: one method using deep learning model to classify behavior and another method making use
            of landmarks and optical flow analysis to characterize behavior. The approach differs for each with different success criteria for each method.
        </p>
        <h3>Deep Learning</h3>
        
        <h3>Landmarks and Optical Flow Analysis</h3>
        <p>
            As an alternative to a pure deep learning procedure, we implement a motion-based approach using optical flow to detect movements using
            facial landmarks. This method analyzes frame-to-frame changes in video captured from a live camera feed or video, 
            allowing us to track  head movement, eye movement, and other facial features without relying on labeled image classification.
        </p>
        <h4>Face Detection</h4>
        <ul>
            <li>Use MediaPipe's pre-trained facial detection model to efficiently locate bounding boxes around the driver's face in each frame</li>
            <li>If there are multiple faces found, select the largest one to filter out other faces and possible false detections.</li>
            <li>Capture the dimensions of this bounding box as the region of interest to work with.</li>
        </ul>
        <br>
        <p>
            Initially, the Haar cascade classifier was used for facial detection, and while it was very fast, it tended to be inaccurate and the bounding box
            that was returned was often to small and did not capture enough of the face. Additionally, there would be many frames where it simply did not capture
            any faces. From a sample of 4089 frames, the Haar cascade classifier could not locate a face in 1315 of these frames, while MediaPipe only missed 72 frames.
        </p>
            
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/facebox_frame_2911_haar.png" alt="Haar" style="width:400px; height:auto;">
                <figcaption>Bounding Box with Haar Cascades</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/facebox_frame_2911_mp.png" alt="MediaPipe" style="width:400px; height:auto;">
                <figcaption>Bounding Box with MediaPipe</figcaption>
            </figure>
        </div>

        <p>
            With the bounding box, we could ignore anything outside of it and use this to reduce computation and possible errors. If there are multiple people in the car,
            the main face in front of the camera, the driver, would be the only one with a facial mesh formed for it, further simplifying computation.
        </p>

        <h4>Optical Flow Analysis</h4>
        <ul>
            <li>Extract facial landmarks from consecutive frames using MediaPipe's face mesh.</li>
            <li>Apply Lucas-Kanade optical flow (cv2.calcOpticalFlowPyrLK) to track landmark displacement between frames.</li>
            <li>Compute average horizontal (dx) and vertical (dy) shifts to infer head movement.</li>
            <li>Threshold displacements to classify transitions in one of the following states:
                <ul>
                    <li>turning left</li>
                    <li>turning right</li>
                    <li>looking up</li>
                    <li>looking down</li>
                </ul>
            </li>
        </ul>
        <br>
        <p>
            The initial approach involved making use of Farneback optical flow, but even with the cropped ROI, this was far too computationally expensive. Running live
            was very slow and eventually crashed, and running with an MP4 file caused crashes as well, doing about 2000 frames in 20 minutes. For this reason, we switched
            to using sparse optical flow, the Lucas-Kanade algorithm, which ran much faster and was usable for live detection. The initial implementation made use of
            cv2.goodFeaturesToTrack, which uses the Shi-Tomasi corner detection algorithm, but the features detected with this were not great (click on the second image to expand).
            Largely due to the use of the NIR image, it detected high intensity locations such as the shiny metal component of the headrest behind the face, and occasionally,
            eyes.
        </p>
        <p>
            Another change in procedure was made here to make use of the MediaPipe face mesh for good features to use for tracking optical flow. This allowed for just enough
            useful features to track facial components and get a good indicator of optical flow between frames, while also being easily run live. Additionally, since just the 
            face was being used for these landmarks, extraneous keypoints from the surroundings that may still appear in the face's bounding box, would not appear. The transition
            state is set based on the average flow in the horizontal and vertical decision, with the horizontal flow taking priority.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/op_flow/op_flow_farneback_frame_46_frame_47.png" alt="Farneback">
                <figcaption>Dense Optical Flow (Farneback)</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/op_flow/op_flow_lkt_frame_46_frame_47.png" alt="Lucas-Kanade (ST)">
                <figcaption>Sparse Optical Flow (Lucas-Kanade) with Shi-Tomasi Corner Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/op_flow/op_flow_lkt-mp_frame_46_frame_47.png" alt="Lucas-Kanade (MP)">
                <figcaption>Sparse Optical Flow (Lucas-Kanade) with MediaPipe Facial Landmarks</figcaption>
            </figure>
        </div>

        <h4>Decision Assignment</h4>
        <ul>
            <li>Calculate eye aspect ratio (EAR) from eye landmarks to determine whether eyes are open or closed.</li>
            <li>Estimate yaw and pitch from nose and eye corner positions to determine head orientation (straight, left, right, down).</li>
            <li>Combine optical flow transitions, EAR values, and head orientation to assign a decision for a frame based on thresholds for each of these.</li>
            <li>Assign final states:
                <ul>
                    <li>good: Eyes are open (or mid-blink) and head is facing straight. Not actively transitioning.</li>
                    <li>careful: Transitory state indicating that driver is not distracted, but appear to be transitioning head state.</li>
                    <li>distracted: Eyes are not open and/or head is not straight.</li>
                </ul>
            </li>
        </ul>
        <br>
        <p>
            This step in the pipeline is fairly straightforward, and largely consists of thresholding EAR values, yaw and pitch values, and making a decision tree to
            determine the final state of the driver. Eye aspect ratio is a formula that was proposed to detect eye closures [3]:
            $$\mathrm{EAR} = \frac{\lVert \mathbf{p}_2 - \mathbf{p}_6 \rVert + \lVert \mathbf{p}_3 - \mathbf{p}_5 \rVert}{2 \lVert \mathbf{p}_1 - \mathbf{p}_4 \rVert}$$
            This allowed us to use an effective and proven method of understanding when the user is closing their eyes on each frame. The optical flow was used to find 
            transition states that cause a frame to be labelled "careful" if it indicated that the original state of the driver's head was facing straight. This allowed for 
            a smoother transition into determining when potentially "distracted" frames were coming up. The yaw and pitch values were derived by
            using the midpoint between the eyes and the nose to determine head orientation. Through trial and error, effective thresholds were found with a light smoothing 
            applied to account for blinking and slight movements that do not fully cause a driver to be distracted, but if maintained for a lenghty amount of time, 
            could represent a distracted driver. One thing to note here is that, unlike the CNNs, there were no specific labels, so it is a combination of these thresholds
            and decision tree to make the binary classification between distracted or not (the "careful" label is ultimately the same as "good").
        </p>
    </section>

    <section class="section" id="experiments">
        <h2>Experiments and Results</h2>
        <h3>Dataset</h3>
        <p>
            For this project we will use the <a href="https://www.kaggle.com/datasets/zeyad1mashhour/driver-inattention-detection-dataset" target="_blank" rel="noopener noreferrer">Driver Inattention Detection Dataset</a> from Kaggle, 
            containing nearly 15,000 NIR images of drivers in 6 different states:
        </p>
        <ul>
            <li>Safe Driving</li>
            <li>Distracted Driving</li>
            <li>Dangerous Driving</li>
            <li>Sleepy Driving</li>
            <li>Drinking</li>
            <li>Yawn</li>
        </ul>
        <p>This dataset is split into three different sets: training, validation, and test. Each set contains the following quantity of test points:</p>
        <ul>
            <li>Training Set: 11,942 images</li>
            <li>Validation Set: 1,922 images</li>
            <li>Test Set: 985 images</li>
        </ul>
        <p>Below are sample frames extracted from the dataset.</p>
        <div class="image-grid">
            <figure>
                <img src="../proposal/static/man_focused.jpg" alt="Safe Driving">
                <figcaption>Safe Driving</figcaption>
            </figure>
            <figure>
                <img src="../proposal/static/man_drinking.jpg" alt="Drinking">
                <figcaption>Drinking</figcaption>
            </figure>
            <figure>
                <img src="../proposal/static/man_on_phone.jpg" alt="Distracted Driving">
                <figcaption>Distracted Driving</figcaption>
            </figure>
            <figure>
                <img src="../proposal/static/man_sleeping.jpg" alt="Sleepy Driving">
                <figcaption>Sleepy Driving</figcaption>
            </figure>
        </div>
        <h3>Experiments</h3>
        <p>
            We have completed three different experimental approach to achieve our overall goal of real-time identification of dangerous driving:
        </p>
        <ul>
            <li>Deep learning using a pre-weighted model (i.e. resnet18)</li>
            <li>Deep learning using a custom-built CNN</li>
            <li>Optical flow analysis</li>
        </ul>
        <h4>Deep Learning (Custom CNN)</h4>
        <p>
            The dataset used here will be the <a href="https://www.kaggle.com/datasets/zeyad1mashhour/driver-inattention-detection-dataset" target="_blank" rel="noopener noreferrer">Driver Inattention Detection Dataset</a> 
            identified earlier in the proposal to train a model using PyTorch. This dataset is already split with 80% dedicated to training and 20% for testing and validation. We hope to achieve at least 85% test accuracy through the tuning of model hyperparameters
            to consider this approach a successful one. Additionally, we want to ensure that this can be set up as a lightweight model to be used in real-time. Performance concerns may also impact overall accuracy when used in
            a realistic scenario.
        </p>
        <h4>Deep Learning (Pre-weighted Model)</h4>
        <p>
            The same dataset that was used for the custom CNN will be used here. We expect that this model will be far more successful than the custom CNN, but we will continue to aim for 85% test accuracy after the transfer
            learning phase. The resnet18 model is fairly lightweight so it should theoretically perform well in real-time. A comparison analysis will be performed between these two deep learning
            approaches to observe how each differs and the results that they achieve.
        </p>
        <h4>Landmarks and Optical Flow Analysis</h4>
        <p>
            This approach was largely qualitative as it was not feasible to manually label each of the thousands of frames for analysis, but we had two main experimental setups for this approach.
            The first was live testing where the code is run and computations happen on the current frame, which performed quite well with no issues in video quality or lag.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/plots/fps_timeline.png" alt="FPS Timeline" style="width:700px; height:auto;">
                <figcaption>FPS Timeline with a Jetson Nano and 720p Webcam</figcaption>
            </figure>
        </div>
        <p>
            Live testing was conducted both on standard PCs and on a Jetson Nano running an Ubuntu image. In all cases, the video playback was smooth with no visible choppiness.
            As seen above, other than a few jumps, the FPS was consistently between 10 and 15 FPS largely as a consequence of how the FPS measurement was done. 
            To highlight the true performance, the smoothed FPS has also been plotted to show the consistent frame rate that this approach was able to achieve.
        </p>
        <p>
            The second experimental setup was using a pre-recorded MP4 file. This was the most used setup as it allowed for a consistent video that could be used to test our different
            approaches and better compare the outputs. Using the optical flow method the video that is in the <a href="#qual-results">Qualitative Results</a> section, we plotted
            the heatmaps seen below.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/plots/focus_state_comparisons.png" alt="Focus State Heatmps">
                <figcaption>Heatmaps of the Characterizing Components vs Focus State</figcaption>
            </figure>
        </div>
        <p>
            As expected, the most common focus states for the head were "good" and "distracted" with the "careful" state allowing for some nuance in the driver's actions. While the
            main strategy was if the head is facing straight and the eyes are open, the user is focused and not distracted, we can see that the user was still determined to be distracted
            if they were facing straight ahead due to eyes closing or lateral motion vectors causing the system to flag that frame as "careful" or possibly even "distracted" if the driver
            was in that position for multiple consecutive frames. Another interesting observation with these heatmaps is with "Transition vs Focus State" where we can see that most of the 
            distractions were made in the section with no transition, indicating that the user spent most of the final decisions were made when the user was not transitioning between head
            orientations
        </p>
        <p>
            As mentioned in the approach, thresholds formed the majority of tunable parameters for this model. While the MediaPipe face detection and the actual Lucas-Kanade optical flow
            functions had many parameters, we set these early on and did not have to vary them to enhance the functionality of the overall detector. The following are the thresholds that
            were manipulated as part of testing:
            <ul>
                <li>Lateral Flow</li>
                <li>Vertical Flow</li>
                <li>EAR</li>
                <li>Yaw</li>
                <li>Pitch</li>
                <li>Eye Closed Count</li>
                <li>Distracted Count</li>
            </ul>
        </p>
        <p>
            Manual testing was used to determine the optimal values for each of these thresholds, with EAR being the most straightforward to determine whether a driver's eyes were
            open or closed. Flow was also fairly straightforward, but did require refinement to ensure that movements that were a very slight adjustment were not captured, while also
            ensuring that larger movements were captured. The current values (of 3 for average lateral flow and 5 for average vertical flow) worked well and were assessed at the same
            time to output one directional label. The lateral directions were given priority as this was where most of the distractions happened, i.e. looking in the upper right would
            give a transition label of "turning right" over "looking up," but this set of labels could easily have been expanded to include more directions. In the images below,
            we see that a lower vertical flow threshold contributes to sensitive vertical transition decisions where visually the driver's head orientation did not seem to change, but
            the detector picks up on the slight downward flow.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/experiments/final_frame_805_3dy.png" alt="3 dy" style="width:400px; height:auto;">
                <figcaption>Test Threshold for Average Vertical Flow of 3</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/experiments/final_frame_805_5dy.png" alt="5 dy" style="width:400px; height:auto;">
                <figcaption>Used Threshold for Average Vertical Flow of 5</figcaption>
            </figure>
        </div>
        <p>
            The counts for eye closures and distractions were originally implemented to allow for the driver to turn left to check the mirrors or capture 
            blinks. If a driver exceeds 30 frames in this middle state, they are considered to be a "distracted" driver. Without these counts, the detector would be very quick to assign
            "detected" decisions, even if the user was simply checking their rear-view mirror.
            Yaw and pitch thresholds were more difficult to determine but the intention was to capture a range of yaw and pitch that would be considered "normal" movement that an 
            undistracted driver would do. This was challenging as different people had different ranges and it was heavily dependent on the camera position. When setting up the camera,
            we had to be sure that the driver was in a fixed position in front of the camera looking straight, otherwise they would appear as slightly leaning in one direction or another,
            causing a "distracted" decision. As seen below, the left image had a very slight left movement that caused the lower yaw threshold of 0.01 to trigger and mark the frame
            as potentially dangeorous, while the yaw threshold that is actually implemented maintained a "good" focus state throughout this.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/experiments/final_frame_1448_0.05yaw.png" alt="0.05 yaw" style="width:400px; height:auto;">
                <figcaption>Test 0.05 Yaw Threshold</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/experiments/final_frame_1448_0.1yaw.png" alt="0.1 yaw" style="width:400px; height:auto;">
                <figcaption>Used 0.1 Yaw Threshold</figcaption>
            </figure>
        </div>
        </p>
        <h4>Final Assessment</h4>
        <p>
            Ultimately, the approach that made use of 
        </p>
    </section>

    <section class="section" id="qual-results">
        <h2>Qualitative Results</h2>
        <h3>Deep Learning</h3>
        <h3>Landmarks and Optical Flow Analysis</h3>
        <p>
            As discussed in the previous section this process includes three steps in the pipeline. In the successful "distracted" classification that follows, note that
            the optical flows inform the system of movement that could contribute to being the movements being a distraction. It is the relation between the position of
            the nose and the midpoint between the eyes that determines the actual direction of the head (left, right, up, down).
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/distracted/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/distracted/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/distracted/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/distracted/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            Similarly, a successful "good" classification is as follows. Here, we see almost no optical flow, indicating no movement anywhere. Since eyes are open and
            the driver is facing straight with acceptable pitch and yaw, this is determined to include no distracted driving.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/good/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            While eyes are a major part of determining whether a driver is distracted (i.e sleepy), the pipeline accounts for blinks by allowing 9 frames before eyes 
            are considered closed, and possibly indicating that the driver is falling asleep.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/good_blink/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good_blink/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good_blink/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/good_blink/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            Besides the eyes, temporal smoothing is also applied when making the decision that a frame indicates distracted driving. In the sequence below, 
            the optical flow suggests an impending turn to the left, but this could be a slight movement that is not necessarily unsafe. However,
            if the driver is in this state for more than 30 frames, it is formally classified as being "distracted" driving. In this example, this was only
            a slight movement and the status went back to "good" after.
        </p>
        <div class="image-grid">
            <figure>
                <img src="./static/optical_flow/careful/original.png" alt="Frame">
                <figcaption>Frame</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/careful/step1.png" alt="Face Detection">
                <figcaption>Step 1: Face Detection</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/careful/step2.png" alt="Optical Flow Computation">
                <figcaption>Step 2: Optical Flow Computation</figcaption>
            </figure>
            <figure>
                <img src="./static/optical_flow/careful/step3.png" alt="Decision Assignment">
                <figcaption>Step 3: Decision Assignment</figcaption>
            </figure>
        </div>
        <p>
            The full video that these frames have come from can be viewed here:
        </p>
        <div class="video-container">
            <video controls>
                <source src="static/optical_flow/final_web.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </section>

    <section class="section" id="conclusion">
        <h2>Conclusion</h2>
    </section>
    
    <section class="section" id="references">
        <h2>References</h2>
        <ol class="ieee-references">
            <li>
                Centers for Disease Control and Prevention, “Distracted Driving,” *CDC*, 2023. [Online]. Available: 
                <a href="https://www.cdc.gov/distracted-driving/about/index.html" target="_blank" rel="noopener noreferrer">
                    https://www.cdc.gov/distracted-driving/about/index.html
                </a>
            </li>
            <li>
                Insurify, “Texting and Driving Statistics,” *Insurify*, 2023. [Online]. Available: 
                <a href="https://insurify.com/car-insurance/insights/texting-and-driving-statistics/" target="_blank" rel="noopener noreferrer">
                    https://insurify.com/car-insurance/insights/texting-and-driving-statistics/
                </a>
            </li>
            <li>
                T. Soukupová and J. Čech, "Real-Time Eye Blink Detection using Facial Landmarks," 
                Center for Machine Perception, Department of Cybernetics, Faculty of Electrical Engineering, 
                Czech Technical University in Prague, Research Report No. 2016-05, 2016. [Online]. Available: 
                <a href="https://cmp.felk.cvut.cz/ftp/articles/cech/Soukupova-TR-2016-05.pdf" target="_blank" rel="noopener noreferrer">
                    https://cmp.felk.cvut.cz/ftp/articles/cech/Soukupova-TR-2016-05.pdf
                </a>
            </li>
        </ol>
    </section>
    <script>
        const images = document.querySelectorAll(".image-grid img");
        const overlay = document.querySelector(".overlay");

        images.forEach(img => {
            img.addEventListener("click", () => {
            if (img.classList.contains("expanded")) {
                img.classList.remove("expanded");
                overlay.style.display = "none";
            } else {
                images.forEach(i => i.classList.remove("expanded"));
                img.classList.add("expanded");
                overlay.style.display = "block";
            }
            });
        });

        overlay.addEventListener("click", () => {
            images.forEach(i => i.classList.remove("expanded"));
            overlay.style.display = "none";
        });
        </script>
</body>
</html>
