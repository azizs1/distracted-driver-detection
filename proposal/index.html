<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project Proposal</title>
    <link rel="stylesheet" href="../css/proposal.css">
</head>
<body>
    <nav class="side-nav">
        <ul>
            <li><a href="#title">Top</a></li>
            <li><a href="#problem">Problem Statement</a></li>
            <li><a href="#approach">Approach</a></li>
            <li><a href="#experiments">Experiments</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </nav>


    <section class="title" id="title">
        <h1>Project Proposal: Detecting Distracted Driving Using Computer Vision Techniques</h1>
        <div class="subtitles">
            <p>Aziz Shaik and Rayden Dodd</p>
            <p>ECE 5554: Computer Vision (Fall 2025)</p>
            <p>Virginia Tech</p>
        </div>
    </section>

    <section class="section" id="problem">
        <h2>Problem Statement</h2>
        <p>
            Distracted driving is a leading cause of accidents around the world, posing a serious threat to public safety.
            According to the Centers for Disease Control and Prevention (CDC), over 3,100 individuals were killed and 
            approximately 424,000 were injured in motor vehicle crashes involving a distracted driver in the United States in 2019 [1]. 
            Equating to almost nine deaths per day, there is a very present need to solve this problem.
            Despite this, studies indicate that 68% of Gen Z drivers admit to using mobile devices while operating a vehicle [2], 
            highlighting a disconnect between the awareness of the risk and the actual driving behavior.
        </p>
        <p>
            Many modern vehicles incorporate systems to ensure drivers are focused by limiting interface access or flashing a warning when
            it detects irregular driving. However, these systems primarily respond to interactions with the vehicle itself and often 
            fail to capture the broader spectrum of risky driving behaviors.
        </p>
        <p>
            The core challenge presented by this problem lies in accurately identifying specific driver behaviors in real time using visual 
            data, with operator actions such as phone usage, eating, or signs of tiredness, and making intelligent decisions based on the 
            severity of the behavior. A system capable of detecting these patterns could help deter dangerous conduct and contribute to a 
            safer travel experience for passengers and nearby drivers.
        </p>
    </section>

    <section class="section" id="approach">
        <h2>Approach</h2>
        <p>
            The technical approach for this problem involves mainly deep learning using convolution neural networks (CNNs).
            Using a dedicated <a href="https://www.kaggle.com/datasets/zeyad1mashhour/driver-inattention-detection-dataset" target="_blank" rel="noopener noreferrer">dataset</a> with labelled frame data
            to train a CNN, we aim to be able to classify images of drivers into one of the 6 available categories to determine when distracted driving is occurring.
            An outline of this approach is as follows:
        </p>
        <h3>Data Preprocessing</h3>
        <ul>
            <li>Normalize pixel values</li>
            <li>Augment data to increase diversity (e.g: rotations, flips)</li>
        </ul>
        <h3>Model Selection</h3>
        <ul>
            <li>Train a basic custom-built CNN model</li>
            <li>Compare our model to a pre weighted model like <a href="https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html" target="_blank" rel="noopener noreferrer">resnet18</a> which is pre trained on the <a href="https://en.wikipedia.org/wiki/ImageNet" target="_blank" rel="noopener noreferrer">ImageNet</a> database. 
                This model is pre trained on 14 million labeled images. We can then use transfer learning with this model to change the final classification layer to suit our labels</li>
            <li>For running on the Jetson Nano, a lightweight model (i.e. MobileNetV3-Small), can be trained for faster live detection.</li>
        </ul>
        <h3>Training and Evaluation</h3>
        <ul>
            <li>Split the dataset into training, validation, and test sets</li>
            <li>Use cross-entropy loss and Adam optimizer for training</li>
            <li>Evaluate model performance using accuracy, precision, recall, and F1-score</li>
        </ul>
        <h3>Deployment</h3>
        <ul>
            <li>Load the trained model onto an NVIDIA Jetson with a camera connected.</li>
            <li>Run a simple script to do live predictions while we act out safe vs. distracted behaviors.</li>
            <li>Show the predicted label on screen and trigger a basic alert during unsafe moments.</li>
        </ul>
    </section>

    <section class="section" id="experiments">
        <h2>Plans for Experiments</h2>
        <p>
            Describe the experimental setup you will follow. Describe the datasets you
            plan to use, what code you will implement yourself, what existing code you will borrow (if any), and
            what you would define as a success for the project. If you intend to collect your own data, provide a
            description of the procedure that you will follow. Provide a list of experiments that you will perform.
            Describe what you expect the experiments to reveal, or what is uncertain about the potential
            outcomes.
        </p>
        <h3>Dataset</h3>
        <p>
            For this project we will use the  <a href="https://www.kaggle.com/datasets/zeyad1mashhour/driver-inattention-detection-dataset" target="_blank" rel="noopener noreferrer">Driver Inattention Detection Dataset</a> from Kaggle, 
            containing nearly 15,000 grayscale images of drivers in 6 different states:
        </p>
        <ul>
            <li>Safe Driving</li>
            <li>Distracted Driving</li>
            <li>Dangerous Driving</li>
            <li>Sleepy Driving</li>
            <li>Drinking</li>
            <li>Yawn</li>
        </ul>
        <p>This dataset is split into three different sets: training, validation, and test. Each set contains the following quantity of test points:</p>
        <ul>
            <li>Training Set: 11,942 images</li>
            <li>Validation Set: 1,922 images</li>
            <li>Test Set: 985 images</li>
        </ul>
        <p>Below are sample frames extracted from the dataset.</p>
        <div class="image-grid">
            <figure>
                <img src="./static/man_focused.jpg" alt="Safe Driving">
                <figcaption>Safe Driving</figcaption>
            </figure>
            <figure>
                <img src="./static/man_drinking.jpg" alt="Drinking">
                <figcaption>Drinking</figcaption>
            </figure>
            <figure>
                <img src="./static/man_on_phone.jpg" alt="Distracted Driving">
                <figcaption>Distracted Driving</figcaption>
            </figure>
            <figure>
                <img src="./static/man_sleeping.jpg" alt="Sleepy Driver">
                <figcaption>Sleepy Driver</figcaption>
            </figure>
        </div>
        <h3>Experiments</h3>
        <p>
        To implement this project ,we will use Python with the pytorch library to build and train our CNN model based on the 6 states given in our dataset.
        We will also use OpenCV to handle image processing and data augmentation.
        Using a the premade data set we already have the data split as follows provided to us:
        </p>
        <p>
            We will use train set and validation set to train and tune our model.
            While using the Testing set to finally evaluate our model on data it hasn't seen before.
            Tunning our model Hyperparameters (Ex: Layers, Epochs, Learning Rate, Batch sizes) to try and reach an accuracy rate of 85% or higher.
            Once trained we will compare it to our pre-weighed models as well to see how they perform.
            We expect that our pre-trained models will outperform our basic CNN model.
        </p>
        <p>
            Next, we will test it live by deploying our best final trained model on to an Nvidia Jetson with a camera.
            Using our own faces mimicking  the different states of safe/distracted driving and seeing if our model can accurately detect our behavior in real time.
        </p>
    </section>
    
    <section class="section" id="references">
        <h2>References</h2>
        <ol class="ieee-references">
            <li>
                Centers for Disease Control and Prevention, “Distracted Driving,” *CDC*, 2023. [Online]. Available: 
                <a href="https://www.cdc.gov/distracted-driving/about/index.html" target="_blank" rel="noopener noreferrer">
                    https://www.cdc.gov/distracted-driving/about/index.html
                </a>
            </li>
            <li>
                Insurify, “Texting and Driving Statistics,” *Insurify*, 2023. [Online]. Available: 
                <a href="https://insurify.com/car-insurance/insights/texting-and-driving-statistics/" target="_blank" rel="noopener noreferrer">
                    https://insurify.com/car-insurance/insights/texting-and-driving-statistics/
                </a>
            </li>
        </ol>
    </section>
</body>
</html>
